{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f8c816f",
   "metadata": {},
   "source": [
    "# A1_5 â€“ Optional Part 2: Cross Validattion\n",
    "\n",
    "We are using the BP from scratch model + KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75b4d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and classes\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import joblib\n",
    "import sys , os\n",
    "base = os.path.dirname(os.getcwd())  \n",
    "sys.path.append(os.path.join(base, \"models\"))\n",
    "sys.path.append(os.path.join(base, \"utils\"))\n",
    "from NeuralNet import NeuralNet\n",
    "from utils import predict_batch, mape, evaluate_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34357710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trainval_np: (1200, 61)\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data from ../data\n",
    "X_trainval_np = np.load(\"../data/X_trainval_np.npy\")\n",
    "y_trainval = np.load(\"../data/y_trainval.npy\")\n",
    "y_trainval_scaled = np.load(\"../data/y_trainval_scaled.npy\")\n",
    "y_scaler = joblib.load(\"../data/y_scaler.joblib\")\n",
    "\n",
    "n_features = X_trainval_np.shape[1]\n",
    "print(\"X_trainval_np:\", X_trainval_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be5b6ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration to evaluate with K-Fold\n",
    "\n",
    "# Set hyperparameters for the evaluations\n",
    "cv_configs = [\n",
    "    {\n",
    "        \"name\": \"One Layer tanh\",\n",
    "        \"hidden_layers\": [20],\n",
    "        \"epochs\": 400,\n",
    "        \"lr\": 0.005,\n",
    "        \"momentum\": 0.5,\n",
    "        \"activation\": \"tanh\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Deep tanh\",\n",
    "        \"hidden_layers\": [60, 30],\n",
    "        \"epochs\": 700,\n",
    "        \"lr\": 0.002,\n",
    "        \"momentum\": 0.9,\n",
    "        \"activation\": \"tanh\",\n",
    "    },\n",
    "]\n",
    "\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32582f30",
   "metadata": {},
   "source": [
    "### K-fold cross-validation for hyperparameter evaluation\n",
    "\n",
    "In this section we use **k-fold cross-validation** to evaluate different neural network configurations.\n",
    "\n",
    "The idea is the following:\n",
    "\n",
    "- We have a list of model configurations (`cv_configs`), with different\n",
    "  hyperparameters (number of layers, learning rate, momentum, activation, etc.).\n",
    "- For each configuration, we use **KFold** to split the training+validation data\n",
    "  into `k_folds` parts.\n",
    "- In every fold, we train the network on `k-1` parts and validate it on the\n",
    "  remaining part.\n",
    "- We repeat this process for all folds, so every sample is used for validation once.\n",
    "\n",
    "For each fold we compute three regression metrics:\n",
    "\n",
    "- **MSE** (Mean Squared Error)  \n",
    "- **MAE** (Mean Absolute Error)  \n",
    "- **MAPE** (Mean Absolute Percentage Error)\n",
    "\n",
    "At the end, we calculate the **mean** and **standard deviation** of these metrics across all folds, for each configuration.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2696120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================\n",
      "Config: One Layer tanh\n",
      "====================================\n",
      "Fold 1/5\n",
      "Neural network has been initialized\n",
      "Architecture (neurons per layer): [61, 20, 1]\n",
      "Activation function used: tanh\n",
      " Layer 1: w(20, 61), theta(20, 1)\n",
      " Layer 2: w(1, 20), theta(1, 1)\n",
      "Epoch 0: Train MSE=0.116081\n",
      "Epoch 100: Train MSE=0.004340\n",
      "Epoch 200: Train MSE=0.001627\n",
      "Epoch 300: Train MSE=0.000871\n",
      "Fold 2/5\n",
      "Neural network has been initialized\n",
      "Architecture (neurons per layer): [61, 20, 1]\n",
      "Activation function used: tanh\n",
      " Layer 1: w(20, 61), theta(20, 1)\n",
      " Layer 2: w(1, 20), theta(1, 1)\n",
      "Epoch 0: Train MSE=0.115141\n",
      "Epoch 100: Train MSE=0.002966\n",
      "Epoch 200: Train MSE=0.001228\n",
      "Epoch 300: Train MSE=0.000702\n",
      "Fold 3/5\n",
      "Neural network has been initialized\n",
      "Architecture (neurons per layer): [61, 20, 1]\n",
      "Activation function used: tanh\n",
      " Layer 1: w(20, 61), theta(20, 1)\n",
      " Layer 2: w(1, 20), theta(1, 1)\n",
      "Epoch 0: Train MSE=0.119714\n",
      "Epoch 100: Train MSE=0.003654\n",
      "Epoch 200: Train MSE=0.001421\n",
      "Epoch 300: Train MSE=0.000770\n",
      "Fold 4/5\n",
      "Neural network has been initialized\n",
      "Architecture (neurons per layer): [61, 20, 1]\n",
      "Activation function used: tanh\n",
      " Layer 1: w(20, 61), theta(20, 1)\n",
      " Layer 2: w(1, 20), theta(1, 1)\n",
      "Epoch 0: Train MSE=0.106229\n",
      "Epoch 100: Train MSE=0.003262\n",
      "Epoch 200: Train MSE=0.001288\n",
      "Epoch 300: Train MSE=0.000685\n",
      "Fold 5/5\n",
      "Neural network has been initialized\n",
      "Architecture (neurons per layer): [61, 20, 1]\n",
      "Activation function used: tanh\n",
      " Layer 1: w(20, 61), theta(20, 1)\n",
      " Layer 2: w(1, 20), theta(1, 1)\n",
      "Epoch 0: Train MSE=0.118224\n",
      "Epoch 100: Train MSE=0.003746\n",
      "Epoch 200: Train MSE=0.001532\n",
      "Epoch 300: Train MSE=0.000916\n",
      "\n",
      "====================================\n",
      "Config: Deep tanh\n",
      "====================================\n",
      "Fold 1/5\n",
      "Neural network has been initialized\n",
      "Architecture (neurons per layer): [61, 60, 30, 1]\n",
      "Activation function used: tanh\n",
      " Layer 1: w(60, 61), theta(60, 1)\n",
      " Layer 2: w(30, 60), theta(30, 1)\n",
      " Layer 3: w(1, 30), theta(1, 1)\n",
      "Epoch 0: Train MSE=0.123119\n",
      "Epoch 100: Train MSE=0.000328\n",
      "Epoch 200: Train MSE=0.000181\n",
      "Epoch 300: Train MSE=0.000116\n",
      "Epoch 400: Train MSE=0.000083\n",
      "Epoch 500: Train MSE=0.000069\n",
      "Epoch 600: Train MSE=0.000047\n",
      "Fold 2/5\n",
      "Neural network has been initialized\n",
      "Architecture (neurons per layer): [61, 60, 30, 1]\n",
      "Activation function used: tanh\n",
      " Layer 1: w(60, 61), theta(60, 1)\n",
      " Layer 2: w(30, 60), theta(30, 1)\n",
      " Layer 3: w(1, 30), theta(1, 1)\n",
      "Epoch 0: Train MSE=0.110331\n",
      "Epoch 100: Train MSE=0.000539\n",
      "Epoch 200: Train MSE=0.000116\n",
      "Epoch 300: Train MSE=0.000051\n",
      "Epoch 400: Train MSE=0.000036\n",
      "Epoch 500: Train MSE=0.000030\n",
      "Epoch 600: Train MSE=0.000026\n",
      "Fold 3/5\n",
      "Neural network has been initialized\n",
      "Architecture (neurons per layer): [61, 60, 30, 1]\n",
      "Activation function used: tanh\n",
      " Layer 1: w(60, 61), theta(60, 1)\n",
      " Layer 2: w(30, 60), theta(30, 1)\n",
      " Layer 3: w(1, 30), theta(1, 1)\n",
      "Epoch 0: Train MSE=0.114858\n",
      "Epoch 100: Train MSE=0.000333\n",
      "Epoch 200: Train MSE=0.000096\n",
      "Epoch 300: Train MSE=0.000050\n",
      "Epoch 400: Train MSE=0.000024\n",
      "Epoch 500: Train MSE=0.000013\n",
      "Epoch 600: Train MSE=0.000010\n",
      "Fold 4/5\n",
      "Neural network has been initialized\n",
      "Architecture (neurons per layer): [61, 60, 30, 1]\n",
      "Activation function used: tanh\n",
      " Layer 1: w(60, 61), theta(60, 1)\n",
      " Layer 2: w(30, 60), theta(30, 1)\n",
      " Layer 3: w(1, 30), theta(1, 1)\n",
      "Epoch 0: Train MSE=0.118402\n",
      "Epoch 100: Train MSE=0.000351\n",
      "Epoch 200: Train MSE=0.000069\n",
      "Epoch 300: Train MSE=0.000028\n",
      "Epoch 400: Train MSE=0.000014\n",
      "Epoch 500: Train MSE=0.000007\n",
      "Epoch 600: Train MSE=0.000005\n",
      "Fold 5/5\n",
      "Neural network has been initialized\n",
      "Architecture (neurons per layer): [61, 60, 30, 1]\n",
      "Activation function used: tanh\n",
      " Layer 1: w(60, 61), theta(60, 1)\n",
      " Layer 2: w(30, 60), theta(30, 1)\n",
      " Layer 3: w(1, 30), theta(1, 1)\n",
      "Epoch 0: Train MSE=0.113936\n",
      "Epoch 100: Train MSE=0.000439\n",
      "Epoch 200: Train MSE=0.000115\n",
      "Epoch 300: Train MSE=0.000071\n",
      "Epoch 400: Train MSE=0.000071\n",
      "Epoch 500: Train MSE=0.000064\n",
      "Epoch 600: Train MSE=0.000054\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Folds</th>\n",
       "      <th>MSE_mean</th>\n",
       "      <th>MSE_std</th>\n",
       "      <th>MAE_mean</th>\n",
       "      <th>MAE_std</th>\n",
       "      <th>MAPE_mean</th>\n",
       "      <th>MAPE_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One Layer tanh</td>\n",
       "      <td>5</td>\n",
       "      <td>0.175663</td>\n",
       "      <td>0.022197</td>\n",
       "      <td>0.312571</td>\n",
       "      <td>0.021065</td>\n",
       "      <td>9.383743</td>\n",
       "      <td>0.712146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deep tanh</td>\n",
       "      <td>5</td>\n",
       "      <td>0.059324</td>\n",
       "      <td>0.006108</td>\n",
       "      <td>0.130005</td>\n",
       "      <td>0.008871</td>\n",
       "      <td>5.323214</td>\n",
       "      <td>0.334790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Name  Folds  MSE_mean   MSE_std  MAE_mean   MAE_std  MAPE_mean  \\\n",
       "0  One Layer tanh      5  0.175663  0.022197  0.312571  0.021065   9.383743   \n",
       "1       Deep tanh      5  0.059324  0.006108  0.130005  0.008871   5.323214   \n",
       "\n",
       "   MAPE_std  \n",
       "0  0.712146  \n",
       "1  0.334790  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # Loop of K-Fold for each configuration\n",
    "\n",
    "# This list will store the average metrics for each model configuration\n",
    "cv_results = []\n",
    "\n",
    "# Loop over every configuration defined in cv_configs\n",
    "for cfg in cv_configs:\n",
    "    print(\"\\n====================================\")\n",
    "    print(\"Config:\", cfg[\"name\"])\n",
    "    print(\"====================================\")\n",
    "\n",
    "    # Lists to save the metrics of each fold\n",
    "    mse_list = []\n",
    "    mae_list = []\n",
    "    mape_list = []\n",
    "\n",
    "    # K-Fold is applied on the training + validation set (80% of the data) kf.split(...) returns the indexes for train and validation in each fold\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_trainval_np), start=1):\n",
    "        print(f\"Fold {fold}/{k_folds}\")\n",
    "\n",
    "        # Select the training and validation features\n",
    "        X_tr = X_trainval_np[train_idx]\n",
    "        X_val = X_trainval_np[val_idx]\n",
    "\n",
    "        # Target for training must stay in scaled form\n",
    "        y_tr_scaled = y_trainval_scaled[train_idx]\n",
    "\n",
    "        # Validation target stays in original scale (\n",
    "        y_val = y_trainval[val_idx]\n",
    "\n",
    "        # Build the network structure: input size -> hidden layers -> 1 output neuron\n",
    "        layers = [n_features] + cfg[\"hidden_layers\"] + [1]\n",
    "\n",
    "        # Create the neural network with the current hyperparameters\n",
    "        net = NeuralNet(\n",
    "            n=layers,                 # network structure\n",
    "            fact=cfg[\"activation\"],   # activation function\n",
    "            eta=cfg[\"lr\"],            # learning rate\n",
    "            alpha=cfg[\"momentum\"],    # momentum value\n",
    "            epochs=cfg[\"epochs\"],     # number of training epochs\n",
    "            val_split=0.0             # no internal validation (we already use K-Fold)\n",
    "        )\n",
    "\n",
    "        # Train the network with the current fold data\n",
    "        net.fit(X_tr, y_tr_scaled)\n",
    "\n",
    "        # Predict on the validation fold (still in scaled form)\n",
    "        y_val_pred_scaled = predict_batch(net, X_val)\n",
    "\n",
    "        # Convert predictions back to the original target scale\n",
    "        y_val_pred = y_scaler.inverse_transform(y_val_pred_scaled).ravel()\n",
    "\n",
    "        # Calculate MSE, MAE and MAPE for this fold\n",
    "        metrics = evaluate_regression(y_val, y_val_pred)\n",
    "\n",
    "        # Save the metrics for this fold\n",
    "        mse_list.append(metrics[\"MSE\"])\n",
    "        mae_list.append(metrics[\"MAE\"])\n",
    "        mape_list.append(metrics[\"MAPE\"])\n",
    "\n",
    "    # After finishing all folds, compute the mean and standard deviation for each metric and save them in the results list\n",
    "    cv_results.append({\n",
    "        \"Name\":      cfg[\"name\"],\n",
    "        \"Folds\":     k_folds,\n",
    "        \"MSE_mean\":  np.mean(mse_list),\n",
    "        \"MSE_std\":   np.std(mse_list),\n",
    "        \"MAE_mean\":  np.mean(mae_list),\n",
    "        \"MAE_std\":   np.std(mae_list),\n",
    "        \"MAPE_mean\": np.mean(mape_list),\n",
    "        \"MAPE_std\":  np.std(mape_list),\n",
    "    })\n",
    "\n",
    "# Create a DataFrame to show the final results in a clean table\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "display(cv_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
