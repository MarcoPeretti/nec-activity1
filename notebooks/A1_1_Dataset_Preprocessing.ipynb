{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d7e12d0",
   "metadata": {},
   "source": [
    "# A1 - Part 1: Dataset selection and preprocessing\n",
    "\n",
    "In this notebook we prepare the dataset that will be used in the rest of the activity.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Load the **Bike Sharing (hourly)** dataset (`hour.csv`).\n",
    "- Explore the original variables and their meaning.\n",
    "- Apply a log transform to the target (`cnt_log`).\n",
    "- One-hot encode categorical variables and build an encoded dataset (`hour_encoded`).\n",
    "- Prepare the final data for the models:\n",
    "  - shuffle and sample 1500 patterns,\n",
    "  - split into 80% train+validation and 20% test,\n",
    "  - scale input features and the target variable.\n",
    "\n",
    "The encoded dataframe and the preprocessed numpy arrays created here will be reused in:\n",
    "\n",
    "- `A1_2_BP_From_Scratch_Experiments.ipynb`\n",
    "- `A1_3_MLR_and_PyTorch_Comparison.ipynb`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d843ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e7cac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour = pd.read_csv('../data/hour.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05f7615",
   "metadata": {},
   "source": [
    "# Day dataframe Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1b640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf36e41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06336468",
   "metadata": {},
   "source": [
    "# Hour dataframe content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4bd0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853e3770",
   "metadata": {},
   "source": [
    "# Dataset format\n",
    "\n",
    "- instant: unique record id\n",
    "- dtday: date in YYYY-MM-DD format\n",
    "- season: [1: spring, 2:summer, 3:fall, 4:winter]\n",
    "- yr: [0: 2011, 1:2012]\n",
    "- mnth: [1 to 12]\n",
    "- hr: hour [0 to 23]\n",
    "- weekday: day of the week [0..6] \n",
    "- holiday: [0: not holiday, 1:holiday]\n",
    "- working day: [0: weeked or holiday 1: not weeked or holiday]\n",
    "- weathersit :\n",
    "  - Clear, Parly Cloudy\n",
    "  - Mist, Cloudy\n",
    "  - Light Snow, Light Rain, Thunderstorm \n",
    "  - Heavy Rain, Snow\n",
    "- temp : Normalized temperature in Celsius (0.0 to 1.0). min=-8, max=+39 \n",
    "- atemp: Normalized feeling temperature in Celsius (0.0 to 1.0). min=-16, max=+50\n",
    "- hum: Normalized humidity (0.0 to 1.0) \n",
    "- windspeed: Normalized wind speed (0.0 to 1.0)\n",
    "- casual: Casual users count\n",
    "- registered: Registered users count\n",
    "- cnt: Rental bikes count (casual + registered)\n",
    "\n",
    "https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b15bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column(s) we don't need\n",
    "hour = hour.drop(columns=['instant', 'dteday'], errors='ignore')\n",
    "\n",
    "# check distinct values\n",
    "print(f'Season: {hour[\"season\"].unique()}')\n",
    "print(f'Year: {hour[\"yr\"].unique()}')\n",
    "print(f'Month: {hour[\"mnth\"].unique()}')\n",
    "print(f'Hour: {hour[\"hr\"].unique()}')\n",
    "print(f'Weekday: {hour[\"weekday\"].unique()}')\n",
    "print(f'Holiday: {hour[\"holiday\"].unique()}')\n",
    "print(f'Working Day: {hour[\"workingday\"].unique()}')\n",
    "print(f'Weathersit: {hour[\"weathersit\"].unique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e937ad9",
   "metadata": {},
   "source": [
    "# Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c6fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hour.isnull().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35168a03",
   "metadata": {},
   "source": [
    "# Outliers\n",
    "From the graph below we can see that there is a large number of bike rentals with low count, but they represent valid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6eede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set(font_scale=1.0)\n",
    "sns.histplot(hour[\"cnt\"], kde=True, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b85523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cnt is higly skewed. Log transform should improve model's accuracy\n",
    "hour['cnt_log'] = np.log1p(hour['cnt'])\n",
    "sns.histplot(hour['cnt_log'], kde=True, bins=50)\n",
    "\n",
    "# Note: we will try predicting cnt_log, and will convert back to cnt using np.expm1(y_pred_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea24310",
   "metadata": {},
   "source": [
    "# Ensure categorical values are represented correctly\n",
    "We one-hot encode categorical values and save the result into a new dataset\n",
    "The intent is to prepare the csv for usage later in the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea82dbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical columns. \n",
    "categorical_cols = ['season', 'yr', 'mnth', 'hr', 'weekday', 'weathersit']\n",
    "\n",
    "# categories have fixed values, known range\n",
    "cat_map = {\n",
    "    \"season\": [1, 2, 3, 4],      \n",
    "    \"yr\": [0, 1],                       \n",
    "    \"mnth\": list(range(1,13)),                              \n",
    "    \"hr\": list(range(24)),                               \n",
    "    \"weekday\": list(range(0,7)),      \n",
    "    \"weathersit\": list(range(1,5))                       \n",
    "}\n",
    "\n",
    "# Format categories for OneHotEncoder (ie: list of category lists)\n",
    "categories_for_encoder = [cat_map[c] for c in categorical_cols]\n",
    "\n",
    "# Build encoder with explicit categories\n",
    "# I don't set ignore error on purpose, as I want to surface them\n",
    "encoder = OneHotEncoder(\n",
    "    categories=categories_for_encoder,\n",
    "    sparse_output=False\n",
    ")\n",
    "\n",
    "# Validate config (from given categories)\n",
    "encoded = encoder.fit_transform(hour[categorical_cols])\n",
    "encoded_cols = encoder.get_feature_names_out(categorical_cols)\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoded_cols, index=hour.index)\n",
    "\n",
    "# Assemble encoded dataset\n",
    "hour_encoded = pd.concat([hour.drop(columns=categorical_cols), encoded_df], axis=1)\n",
    "\n",
    "# Save the encoded dataset\n",
    "hour_encoded.to_csv(\"../data/hours_encoded.csv\", index=False) \n",
    "\n",
    "hour_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ee5219",
   "metadata": {},
   "source": [
    "## Part 1.2 – Preparing the preprocessed dataset for the models\n",
    "\n",
    "Now that we have the encoded dataset `hour_encoded`, we prepare the data in the exact\n",
    "format that will be used by all the models (MLR, manual BP, PyTorch):\n",
    "\n",
    "- select the regression target `cnt_log`,\n",
    "- drop the original `cnt` column from the input features,\n",
    "- randomly sample 1500 patterns,\n",
    "- split into 80% train+validation and 20% test,\n",
    "- apply feature scaling to inputs and target.\n",
    "\n",
    "The resulting NumPy arrays will be used directly in the following notebooks of the assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e87c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Start from the encoded dataset\n",
    "# Copy created in order to not modify the original reference\n",
    "data = hour_encoded.copy()\n",
    "\n",
    "print(\"Encoded dataset shape:\", data.shape)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690c2051",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining target and feature matrix\n",
    "\n",
    "target_col = \"cnt_log\"\n",
    "\n",
    "# We can safely drop the original 'cnt' if it is still present, because we are going to predict cnt_log instead.\n",
    "if \"cnt\" in data.columns:\n",
    "    data = data.drop(columns=[\"cnt\"])\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X_df = data.drop(columns=[target_col])\n",
    "y = data[target_col].values\n",
    "\n",
    "print(\"Feature matrix shape:\", X_df.shape)\n",
    "print(\"Target vector shape :\", y.shape)\n",
    "print(\"Number of features  :\", X_df.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9a07ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Shuffle and sample 1500 patterns. We randomly sample 1500 rows to match the experiments\n",
    "\n",
    "data_sampled = data.sample(n=1500, random_state=42).reset_index(drop=True)\n",
    "\n",
    "X_df = data_sampled.drop(columns=[target_col])\n",
    "y = data_sampled[target_col].values\n",
    "\n",
    "print(\"After sampling 1500 rows:\")\n",
    "print(\"Feature matrix shape:\", X_df.shape)\n",
    "print(\"Target vector shape :\", y.shape)\n",
    "\n",
    "\n",
    "# Train / Test split (80% / 20%)\n",
    "\n",
    "X_trainval_df, X_test_df, y_trainval, y_test = train_test_split(\n",
    "    X_df, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"\\nTrain+Val size:\", X_trainval_df.shape[0])\n",
    "print(\"Test size     :\", X_test_df.shape[0])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_trainval = X_trainval_df.values\n",
    "X_test = X_test_df.values\n",
    "\n",
    "print(\"\\nX_trainval shape:\", X_trainval.shape)\n",
    "print(\"X_test shape     :\", X_test.shape)\n",
    "print(\"y_trainval shape :\", y_trainval.shape)\n",
    "print(\"y_test shape     :\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359238e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature scaling (StandardScaler for X). We scale the input features so that all models (MLR, manual BP, PyTorch) use the same normalized data.\n",
    "\n",
    "x_scaler = StandardScaler()\n",
    "X_trainval_np = x_scaler.fit_transform(X_trainval)\n",
    "X_test_np     = x_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Target scaling (StandardScaler for y). It's also needed to scale the target value\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_trainval_scaled = y_scaler.fit_transform(y_trainval.reshape(-1, 1)).ravel()\n",
    "y_test_scaled     = y_scaler.transform(y_test.reshape(-1, 1)).ravel()\n",
    "\n",
    "n_features = X_trainval_np.shape[1]\n",
    "print(\"Number of input features:\", n_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a1af8f",
   "metadata": {},
   "source": [
    "## Summary of preprocessing\n",
    "\n",
    "In this notebook we combined two main steps:\n",
    "\n",
    "1. **Data understanding and encoding** (first part, by my teammate):\n",
    "   - Loaded the original Bike Sharing hourly dataset (`hour.csv`).\n",
    "   - Removed unnecessary columns.\n",
    "   - Inspected categorical variables and their value ranges.\n",
    "   - Checked for missing values.\n",
    "   - Observed that `cnt` is highly skewed and created a log-transformed target `cnt_log`.\n",
    "   - One-hot encoded categorical variables into a new dataframe `hour_encoded`.\n",
    "   - Saved the encoded dataset into `hours_encoded.csv`.\n",
    "\n",
    "2. **Preparing the final dataset for the models** (second part):\n",
    "   - Started from `hour_encoded`.\n",
    "   - Selected `cnt_log` as the regression target and dropped `cnt`.\n",
    "   - Randomly sampled 1500 patterns for the experiments.\n",
    "   - Split the data into:\n",
    "     - 80% for training + validation (`X_trainval`, `y_trainval`),\n",
    "     - 20% for test (`X_test`, `y_test`).\n",
    "   - Applied **StandardScaler** to:\n",
    "     - the input features (`X_trainval_np`, `X_test_np`),\n",
    "     - the target values (`y_trainval_scaled`, `y_test_scaled`).\n",
    "\n",
    "The following variables are now ready to be used in the next notebooks:\n",
    "\n",
    "- `X_trainval_np`, `X_test_np`  → scaled input features\n",
    "- `y_trainval`, `y_test`        → original target values\n",
    "- `y_trainval_scaled`, `y_test_scaled` → scaled target values\n",
    "- `x_scaler`, `y_scaler`        → fitted scalers (for inverse transformation)\n",
    "- `n_features`                  → number of input features\n",
    "\n",
    "These will be reused in:\n",
    "- **A1_2_BP_From_Scratch_Experiments.ipynb** (manual backprop network)\n",
    "- **A1_3_MLR_and_PyTorch_Comparison.ipynb** (MLR and PyTorch models).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
