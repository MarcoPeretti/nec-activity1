{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc67f53",
   "metadata": {},
   "source": [
    "# Understanding Our Custom Neural Network (Backpropagation From Scratch)\n",
    "\n",
    "This notebook gives a simple and clear explanation of how our custom neural network \n",
    "works. The goal is to understand the logic of the Backpropagation algorithm.\n",
    "\n",
    "This document is for us, to make sure we both understand the model in a high-level way before doing deeper analysis or comparisons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceaec15",
   "metadata": {},
   "source": [
    "## 1. What is our model?\n",
    "\n",
    "We built a **multilayer neural network** using only basic Python and NumPy.\n",
    "We did NOT use TensorFlow, PyTorch, or any machine learning library.\n",
    "\n",
    "This means:\n",
    "- We created the layers ourselves.\n",
    "- We store and update all the weights manually.\n",
    "- We run forward and backward propagation step by step.\n",
    "- We update the weights after each training sample.\n",
    "\n",
    "This model is trained using **online backpropagation**, which means one row at a time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f2ee34",
   "metadata": {},
   "source": [
    "## 2. Very high-level view of how the network learns\n",
    "\n",
    "Here is the simple idea:\n",
    "\n",
    "1. Take one row of data.\n",
    "2. Send the input through the network (forward propagation).\n",
    "3. The network makes a prediction.\n",
    "4. Compare the prediction with the real value and compute the error.\n",
    "5. Send the error backwards through the network (backpropagation).\n",
    "6. Update the weights a little bit.\n",
    "7. Repeat for the next row.\n",
    "8. Repeat the whole process for many epochs.\n",
    "\n",
    "This cycle is the core of neural network learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2f0537",
   "metadata": {},
   "source": [
    "## 3. Forward propagation (step-by-step)\n",
    "\n",
    "Forward propagation means \"send information forward\".\n",
    "\n",
    "For each layer:\n",
    "\n",
    "1. The layer receives inputs (numbers).\n",
    "2. The layer multiplies them by weights.\n",
    "3. It adds a bias.\n",
    "4. It applies an activation function:\n",
    "   - tanh\n",
    "   - sigmoid\n",
    "   - ReLU\n",
    "5. The result becomes the input to the next layer.\n",
    "\n",
    "At the last layer (the output layer) we get one final predicted value.\n",
    "\n",
    "Forward propagation answers the question:\n",
    "> \"Given this input, what does the network think the output should be?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6cefad",
   "metadata": {},
   "source": [
    "## 4. The error\n",
    "\n",
    "After the network produces a prediction `ŷ`, we compare it with the real value `y`.\n",
    "\n",
    "The difference between them is the **error**.\n",
    "\n",
    "The error tells the network:\n",
    "- if the prediction was correct,\n",
    "- and how far it was from the real value.\n",
    "\n",
    "Without this error, the network has no way to learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7dab70",
   "metadata": {},
   "source": [
    "## 5. Backpropagation (the learning step)\n",
    "\n",
    "Backpropagation sends the error **backwards** through the network.\n",
    "\n",
    "What is the idea?\n",
    "\n",
    "- We want to know how much each neuron and each weight contributed to the error.\n",
    "- The network calculates a \"responsibility\" score for every weight.\n",
    "- These scores are called **gradients**.\n",
    "\n",
    "Then we update the weights in the opposite direction of the error:\n",
    "- If a weight increased the error → we decrease it.\n",
    "- If a weight reduced the error → we increase it.\n",
    "\n",
    "We repeat this after every training example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036f3678",
   "metadata": {},
   "source": [
    "## 6. How weights are updated\n",
    "\n",
    "After backpropagation, we have gradients.\n",
    "We use them to update the weights.\n",
    "\n",
    "Our implementation uses:\n",
    "\n",
    "- **Learning rate (eta)**: controls how big the update is.\n",
    "- **Momentum (alpha)**: smooths the updates and helps avoid oscillations.\n",
    "\n",
    "The actual update for each weight is:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52e8df0",
   "metadata": {},
   "source": [
    "## 7. What happens in one epoch?\n",
    "\n",
    "One epoch means:\n",
    "\n",
    "> \"We go through all rows of the training dataset once.\"\n",
    "\n",
    "Inside the epoch:\n",
    "\n",
    "For each row:\n",
    "- forward propagation\n",
    "- compute error\n",
    "- backpropagation\n",
    "- update weights\n",
    "\n",
    "At the end of the epoch:\n",
    "- we can measure the total training error\n",
    "- we can measure the validation error (if val_split > 0)\n",
    "\n",
    "Then we start the next epoch with slightly updated weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c035db",
   "metadata": {},
   "source": [
    "## 8. Internal validation inside the network\n",
    "\n",
    "Our NeuralNet class has a parameter called `val_split`.\n",
    "\n",
    "If val_split > 0:\n",
    "\n",
    "- The input dataset is automatically divided into:\n",
    "  - internal training set\n",
    "  - internal validation set\n",
    "\n",
    "The network:\n",
    "- uses the training part for learning,\n",
    "- and uses the validation part to check generalization.\n",
    "\n",
    "The method `loss_epochs()` returns the training and validation error per epoch.\n",
    "This helps us see if the model is overfitting or learning properly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc65c20",
   "metadata": {},
   "source": [
    "## 9. Summary (simple and complete)\n",
    "\n",
    "Our custom neural network works like this:\n",
    "\n",
    "1. We give it training data.\n",
    "2. For each row:\n",
    "   - forward pass\n",
    "   - compute error\n",
    "   - backpropagate the error\n",
    "   - update weights\n",
    "3. Repeat for many epochs.\n",
    "4. Use validation to check generalization.\n",
    "5. Use test data to evaluate final performance.\n",
    "\n",
    "This is the classic backpropagation algorithm, implemented fully from scratch,\n",
    "using only NumPy and our own code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8f7813",
   "metadata": {},
   "source": [
    "## 10. Why this notebook exists\n",
    "\n",
    "This notebook is designed to help us talk about the model, understand the logic,\n",
    "and make sure both of us feel comfortable explaining how backpropagation works\n",
    "during the interview or in the written report.\n",
    "\n",
    "There is no code here because the goal is understanding, not implementation details.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
