{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c77b209e",
   "metadata": {},
   "source": [
    "# A1_2 â€“ Manual Backpropagation Neural Network (Experiments)\n",
    "\n",
    "In this notebook we:\n",
    "\n",
    "- Load the preprocessed and encoded datasets (`./data`).\n",
    "- Run all the experiments with different neural network hyperparameters\n",
    "  (number of hidden layers, activation function, learning rate, momentum, epochs).\n",
    "- Compare the experiments using:\n",
    "  - MSE, MAE and MAPE on train+validation (80%) and test (20%).\n",
    "- Select one configuration as the \"best\" manual BP model,\n",
    "  which will later be compared against MLR and PyTorch in a separate notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6531b15",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m      6\u001b[39m sys.path.append(\u001b[33m\"\u001b[39m\u001b[33m../models\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m predict_batch, mape, evaluate_regression\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mNeuralNet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NeuralNet \u001b[38;5;66;03m# custom BP implementation (from scratch)\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, mean_absolute_error\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import sys\n",
    "sys.path.append(\"../models\")\n",
    "\n",
    "from utils import predict_batch, mape, evaluate_regression\n",
    "from NeuralNet import NeuralNet # custom BP implementation (from scratch)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "774783e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded preprocessed data from ./data\n",
      "X_trainval_np: (1200, 61)\n",
      "X_test_np    : (300, 61)\n",
      "n_features   : 61\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load preprocessed data from ./data\n",
    "\n",
    "X_trainval_np = np.load(\"../data/X_trainval_np.npy\")\n",
    "X_test_np     = np.load(\"../data/X_test_np.npy\")\n",
    "\n",
    "y_trainval = np.load(\"../data/y_trainval.npy\")\n",
    "y_test     = np.load(\"../data/y_test.npy\")\n",
    "\n",
    "y_trainval_scaled = np.load(\"../data/y_trainval_scaled.npy\")\n",
    "y_test_scaled     = np.load(\"../data/y_test_scaled.npy\")\n",
    "\n",
    "x_scaler = joblib.load(\"../data/x_scaler.joblib\")\n",
    "y_scaler = joblib.load(\"../data/y_scaler.joblib\")\n",
    "\n",
    "n_features = X_trainval_np.shape[1]\n",
    "print(\"Loaded preprocessed data from ./data\")\n",
    "print(\"X_trainval_np:\", X_trainval_np.shape)\n",
    "print(\"X_test_np    :\", X_test_np.shape)\n",
    "print(\"n_features   :\", n_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ff2d157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(hidden_layers, epochs, lr, momentum,\n",
    "                   activation='tanh', val_split=0.2):\n",
    "    \"\"\"\n",
    "    Runs one BP experiment on the Bike Sharing data using the custom NeuralNet.\n",
    "\n",
    "    Outside the NeuralNet class:\n",
    "      - We split the dataset into:\n",
    "            80% -> train+validation (X_trainval_np, y_trainval_scaled)\n",
    "            20% -> test           (X_test_np, y_test_scaled)\n",
    "\n",
    "    Inside the NeuralNet class:\n",
    "      - val_split controls the percentage of validation inside the 80%.\n",
    "      - The class automatically shuffles and splits the 80% into:\n",
    "            (1 - val_split) -> internal training\n",
    "            val_split       -> internal validation\n",
    "\n",
    "    This function:\n",
    "      - trains the network on the 80% (with internal train/val),\n",
    "      - computes predictions on the 80% and on the 20% test,\n",
    "      - returns metrics in the original target scale (cnt_log).\n",
    "    \"\"\"\n",
    "\n",
    "    # Build full layer structure: [input, hidden..., output]\n",
    "    layers = [n_features] + hidden_layers + [1]\n",
    "\n",
    "    # Initialize and train network\n",
    "    net = NeuralNet(\n",
    "        n=layers,\n",
    "        fact=activation,\n",
    "        eta=lr,\n",
    "        alpha=momentum,\n",
    "        epochs=epochs,\n",
    "        val_split=val_split\n",
    "    )\n",
    "\n",
    "    # Train on the 80% (network internally splits into train/val)\n",
    "    net.fit(X_trainval_np, y_trainval_scaled)\n",
    "\n",
    "    # Error evolution per epoch (from NeuralNet)\n",
    "    train_err, val_err = net.loss_epochs()\n",
    "\n",
    "    # Predictions in scaled space \n",
    "    y_trainval_pred_scaled = predict_batch(net, X_trainval_np)\n",
    "    y_test_pred_scaled     = predict_batch(net, X_test_np)\n",
    "\n",
    "    # Back to original target scale (cnt_log) \n",
    "    y_trainval_pred = y_scaler.inverse_transform(y_trainval_pred_scaled).ravel()\n",
    "    y_test_pred     = y_scaler.inverse_transform(y_test_pred_scaled).ravel()\n",
    "\n",
    "    # Metrics (in original cnt_log scale)\n",
    "    trainval_mse  = mean_squared_error(y_trainval, y_trainval_pred)\n",
    "    test_mse      = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "    trainval_mae  = mean_absolute_error(y_trainval, y_trainval_pred)\n",
    "    test_mae      = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "    trainval_mape = mape(y_trainval, y_trainval_pred)\n",
    "    test_mape     = mape(y_test, y_test_pred)\n",
    "\n",
    "    res = {\n",
    "        \"model\": net,\n",
    "        \"train_err\": train_err,\n",
    "        \"val_err\": val_err,\n",
    "        \"Number of layers\": len(layers),\n",
    "        \"Layer Structure\": layers,\n",
    "        \"Num epochs\": epochs,\n",
    "        \"Learning Rate\": lr,\n",
    "        \"Momentum\": momentum,\n",
    "        \"Activation function\": activation,\n",
    "\n",
    "        # Metrics on the 80% (train+val combined)\n",
    "        \"TRAINVAL_MSE\": trainval_mse,\n",
    "        \"TRAINVAL_MAE\": trainval_mae,\n",
    "        \"TRAINVAL_MAPE\": trainval_mape,\n",
    "\n",
    "        # Metrics on the 20% test set\n",
    "        \"TEST_MSE\": test_mse,\n",
    "        \"TEST_MAE\": test_mae,\n",
    "        \"TEST_MAPE\": test_mape,\n",
    "    }\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6393020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter configurations  - Here we'll add all the experiments we want. Target for the activity is to do at least 10\n",
    "hyperparams_list = [\n",
    "\n",
    "    #  1: One Layer tanh \n",
    "    {\n",
    "        \"hidden_layers\": [20],\n",
    "        \"epochs\": 400,\n",
    "        \"lr\": 0.005,\n",
    "        \"momentum\": 0.5,\n",
    "        \"activation\": \"tanh\"\n",
    "    },\n",
    "\n",
    "    #  2: Deep tanh \n",
    "    {\n",
    "        \"hidden_layers\": [40, 15],\n",
    "        \"epochs\": 600,\n",
    "        \"lr\": 0.005,\n",
    "        \"momentum\": 0.9,\n",
    "        \"activation\": \"tanh\"\n",
    "    },\n",
    "\n",
    "    #  3: One Layer sigmoid \n",
    "    {\n",
    "        \"hidden_layers\": [20],\n",
    "        \"epochs\": 400,\n",
    "        \"lr\": 0.005,\n",
    "        \"momentum\": 0.5,\n",
    "        \"activation\": \"sigmoid\"\n",
    "    },\n",
    "\n",
    "    #  4: Deep ReLU \n",
    "    {\n",
    "        \"hidden_layers\": [40, 15],\n",
    "        \"epochs\": 600,\n",
    "        \"lr\": 0.005,\n",
    "        \"momentum\": 0.9,\n",
    "        \"activation\": \"relu\"\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517082bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================\n",
      "Running Experiment 1\n",
      "Config: {'hidden_layers': [20], 'epochs': 400, 'lr': 0.005, 'momentum': 0.5, 'activation': 'tanh'}\n",
      "====================================\n",
      "Neural network has been initialized\n",
      "Architecture (neurons per layer): [61, 20, 1]\n",
      "Activation function used: tanh\n",
      " Layer 1: w(20, 61), theta(20, 1)\n",
      " Layer 2: w(1, 20), theta(1, 1)\n",
      "Epoch 0: Train MSE=0.123519 | Val MSE=0.051414\n",
      "Epoch 100: Train MSE=0.003998 | Val MSE=0.024377\n",
      "Epoch 200: Train MSE=0.001916 | Val MSE=0.028811\n",
      "Epoch 300: Train MSE=0.001107 | Val MSE=0.032472\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "for i, cfg in enumerate(hyperparams_list, start=1):\n",
    "    print(\"\\n====================================\")\n",
    "    print(f\"Running Experiment {i}\")\n",
    "    print(\"Config:\", cfg)\n",
    "    print(\"====================================\")\n",
    "\n",
    "    res = run_experiment(\n",
    "        hidden_layers=cfg[\"hidden_layers\"],\n",
    "        epochs=cfg[\"epochs\"],\n",
    "        lr=cfg[\"lr\"],\n",
    "        momentum=cfg[\"momentum\"],\n",
    "        activation=cfg[\"activation\"],\n",
    "        val_split=0.2      # keep same val_split for all\n",
    "    )\n",
    "\n",
    "    res[\"Experiment\"] = i\n",
    "    all_results.append(res)\n",
    "\n",
    "# Build a DataFrame with the most important fields\n",
    "cols = [\n",
    "    \"Experiment\",\n",
    "    \"Number of layers\",\n",
    "    \"Layer Structure\",\n",
    "    \"Num epochs\",\n",
    "    \"Learning Rate\",\n",
    "    \"Momentum\",\n",
    "    \"Activation function\",\n",
    "    \"TRAINVAL_MSE\", \"TRAINVAL_MAE\", \"TRAINVAL_MAPE\",\n",
    "    \"TEST_MSE\", \"TEST_MAE\", \"TEST_MAPE\",\n",
    "]\n",
    "\n",
    "results_df = pd.DataFrame(all_results)[cols]\n",
    "\n",
    "print(\"\\n=== Summary of all manual BP experiments ===\")\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5202d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select the experiment to use the configuration for the graphs\n",
    "chosen_experiment = 1   # Detail here the Experiment number\n",
    "chosen_idx = results_df.index[results_df[\"Experiment\"] == chosen_experiment][0]\n",
    "\n",
    "print(\"\\nUsing experiment\", chosen_experiment, \"as the selected configuration.\")\n",
    "\n",
    "best_res = all_results[chosen_idx]\n",
    "best_model = best_res[\"model\"]\n",
    "best_train_err = best_res[\"train_err\"]\n",
    "best_val_err = best_res[\"val_err\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2aa4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loss curves for the selected experiment\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(best_train_err, label=\"Train MSE\")\n",
    "if any(e is not None for e in best_val_err):\n",
    "    plt.plot([e for e in best_val_err if e is not None], label=\"Val MSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(f\"Manual NeuralNet - Loss per epoch (Experiment {chosen_experiment})\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f39a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scatter plot: true vs predicted on TEST set\n",
    "\n",
    "\n",
    "# Predictions in scaled space\n",
    "y_test_pred_scaled = predict_batch(best_model, X_test_np)\n",
    "\n",
    "# Back to original target scale\n",
    "y_test_pred = y_scaler.inverse_transform(y_test_pred_scaled).ravel()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.5)\n",
    "plt.xlabel(\"True cnt_log (test)\")\n",
    "plt.ylabel(\"Predicted cnt_log (test)\")\n",
    "plt.title(f\"Manual NeuralNet - True vs Predicted (Experiment {chosen_experiment})\")\n",
    "plt.plot([y_test.min(), y_test.max()],\n",
    "         [y_test.min(), y_test.max()],\n",
    "         linestyle=\"--\")  # ideal y = x line\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a71990",
   "metadata": {},
   "source": [
    "## Summary of manual BP experiments\n",
    "\n",
    "In this notebook we:\n",
    "\n",
    "- Loaded the datasets from (`../data`).\n",
    "- Defined several neural network configurations for the manual BP model\n",
    "  (different number of hidden units, activation functions, learning rate, momentum, epochs).\n",
    "- For each configuration we trained the model and computed:\n",
    "  - MSE, MAE and MAPE on the 80% (train+validation),\n",
    "  - MSE, MAE and MAPE on the 20% test set.\n",
    "- Collected the results in a comparison table and selected one experimentaccording to its performance on the test set.\n",
    "\n",
    "The selected configuration will be used later to compare:\n",
    "\n",
    "- Manual BP vs Multiple Linear Regression (MLR),\n",
    "- Manual BP vs PyTorch neural network\n",
    "\n",
    "in the next notebook: **A1_3_MLR_and_PyTorch_Comparison.ipynb**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
